{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLKL47bu5Udh"
      },
      "source": [
        "# **Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mRblw_rxeoLm",
        "outputId": "677c7e66-a1e1-46f2-e211-00a15cefc8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.7/901.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m713.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSat Dec 21 21:29:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 11.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics -q\n",
        "!pip install facenet-pytorch -q\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!nvidia-smi\n",
        "!pip install --upgrade Pillow -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qQsYjRGJOj"
      },
      "source": [
        "# **Clone facenet-pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B4KOINF9077k",
        "outputId": "443fcdb8-07dc-45bb-d27e-9b24d57410f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'facenet-pytorch'...\n",
            "remote: Enumerating objects: 1338, done.\u001b[K\n",
            "remote: Counting objects: 100% (289/289), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 1338 (delta 236), reused 219 (delta 219), pack-reused 1049 (from 4)\u001b[K\n",
            "Receiving objects: 100% (1338/1338), 23.19 MiB | 12.79 MiB/s, done.\n",
            "Resolving deltas: 100% (656/656), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/timesler/facenet-pytorch.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVjeK7x5aaR"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-8OQdM33VVJ",
        "outputId": "5f689714-a714-47e4-86ff-1f250f923b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import deque, Counter\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL4EjLrfnGo5"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/timesler/facenet-pytorch.git\n",
        "# cd /content/facenet-pytorch\n",
        "# !pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K60NA0oZFRH4"
      },
      "source": [
        "# **Initializing Face Detector and Recognizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAfvg-EAO-sb"
      },
      "source": [
        "InceptionResnetV1 model with vggface2 pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d7bc9694f3c242959e7ede7b3dbab621",
            "802aedba2a474837a1366a681c66ab34",
            "bf2b53d914484a4c8c844e9c5db430a7",
            "a16610475c084149a0d20c89b55ec85f",
            "3c536faa49694d948a4a71c0affd95b4",
            "f5bc0503e5d04c39abb4ab10d5097b6b",
            "9a6ff4074032408c80afada76f7074b1",
            "7d918e81445740059b871a95860d4cab",
            "c9e06fac4bd94cbca0c0a1bd88e36e62",
            "353869a0cc2b458ba68aa7c9469a3a68",
            "46ce5e005413470881aaeca1225ab9eb"
          ]
        },
        "id": "vAcSu8LJl3-m",
        "outputId": "6873a151-67aa-4806-fec5-a8a354253c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device:  cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7bc9694f3c242959e7ede7b3dbab621",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device: \", device)\n",
        "face_detector = YOLO(\"/content/yolov8n-face-keypoints.pt\")\n",
        "face_detector.to(device)\n",
        "face_recognizer = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DAQLoNqFqbD"
      },
      "source": [
        "# Other options: MTCNN or casia-webface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aebca98d303642378f3e35997a1ac7fc",
            "d9f33df4b9bd453ba03d0b59e9310565",
            "5fcbf9e3789d401fbec572cc12f0fe98",
            "5fd04b161634444e99254acc7450b813",
            "9dae755a834143cb9babaa1ba0265620",
            "47bab7a2395c46c2a55adb5d07612d3f",
            "18974c3603544632a35480031e9f0dd6",
            "28e6e2a5593e41848e686d110ff79b28",
            "c0051db1596b4fabaa20f177dbb6297b",
            "294e2c9a484c48f282f2c4eef931149d",
            "fc73dd8129de4241a67d98c24e6a15b4"
          ]
        },
        "id": "B5JxbVuQ01CV",
        "outputId": "66a4cd15-978a-44f7-fe85-1e3f9236194d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aebca98d303642378f3e35997a1ac7fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # # If required, create a face detection pipeline using MTCNN:\n",
        "# # mtcnn = MTCNN(image_size=<image_size>, margin=<margin>)\n",
        "# mtcnn = MTCNN(image_size=\"160\", keep_all=True, thresholds=[0.1, 0.1, 0.1])\n",
        "\n",
        "# # For a model pretrained on CASIA-Webface\n",
        "# face_recognizer = InceptionResnetV1(pretrained='casia-webface').eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWIfXHxHGduk"
      },
      "source": [
        "# **Create database embedding tensor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TX-_lVvK-QTT"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),  # Resize the image to a fixed size\n",
        "    transforms.ToTensor(),           # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize the tensor\n",
        "])\n",
        "\n",
        "img_names_ls = [f for f in os.listdir(\"/content/database\") if f.endswith('.jpg')]\n",
        "\n",
        "img_embedding_list = []\n",
        "\n",
        "for image_name in img_names_ls:\n",
        "  image_path = os.path.join(\"/content/database/\", image_name)\n",
        "  img = cv2.imread(image_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = Image.fromarray(img)\n",
        "  # img = Image.open(image_path).convert('RGB')\n",
        "  img_tensor = transform(img)\n",
        "  img_tensor = img_tensor.to(device)\n",
        "  # Add a batch dimension\n",
        "  img_tensor = img_tensor.unsqueeze(0)  # Shape: (1, 3, 160, 160)\n",
        "  img_embedding = face_recognizer(img_tensor)\n",
        "  img_embedding_list.append((img_embedding/img_embedding.norm(dim=1)))\n",
        "\n",
        "\n",
        "Data_Base = torch.cat(img_embedding_list, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ONwMqs3H9NE"
      },
      "source": [
        "# **Face Crop Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n0d1jP1rABVB"
      },
      "outputs": [],
      "source": [
        "def face_crop_embedder(cropped_face: np.ndarray) -> torch.Tensor:\n",
        "  \n",
        "  cropped_face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "  cropped_face_pil = Image.fromarray(cropped_face_rgb)\n",
        "  cropped_face_tensor = transform(cropped_face_pil)\n",
        "  cropped_face_embedding = face_recognizer(cropped_face_tensor.unsqueeze(0).to(device))\n",
        "  cropped_face_embedding = cropped_face_embedding/cropped_face_embedding.norm(dim=1)\n",
        "\n",
        "  return cropped_face_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SexsDYHONHT"
      },
      "outputs": [],
      "source": [
        "## Face recognition pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4lt7dgZOPUd"
      },
      "source": [
        "#**Face recognition pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IORzAddXNxKP"
      },
      "source": [
        "## **Without Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrDy2oGrNul9"
      },
      "outputs": [],
      "source": [
        "# Open the camera or video file\n",
        "cap = cv2.VideoCapture(\"/content/subway.mp4\")\n",
        "\n",
        "# Initialize other variables\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "# cam_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "cam_fps = 15\n",
        "\n",
        "# Font settings\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 0.8\n",
        "thickness = 2\n",
        "\n",
        "# Output video\n",
        "out = cv2.VideoWriter('/content/result.mp4', cv2.VideoWriter_fourcc(*'mp4v'), cam_fps, (frame_width, frame_height))\n",
        "\n",
        "# simmularity threshold\n",
        "diff_threshold = 0.7\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "\n",
        "        results = face_detector.track(frame, persist=True, device=device)\n",
        "\n",
        "        for r in results[0]:\n",
        "            if 0.6 < r.boxes.conf.item():\n",
        "                points = r.keypoints.xy.cpu().numpy()[0]\n",
        "                x_c, y_c, w_b, h_b = r.boxes.xywh.cpu().numpy()[0]\n",
        "                x_max = int(x_c + (w_b / 2))\n",
        "                x_min = int(x_c - (w_b / 2))\n",
        "                y_max = int(y_c + (h_b / 2))\n",
        "                y_min = int(y_c - (h_b / 2))\n",
        "                \n",
        "                cropped_face = frame[y_min:y_max, x_min:x_max]\n",
        "                cropped_face_embedding = face_crop_embedder(cropped_face)\n",
        "\n",
        "                simularities = (Data_Base*cropped_face_embedding).sum(dim=1)\n",
        "                simularities = 1 - simularities\n",
        "                min_diff_index = torch.argmin(simularities)\n",
        "\n",
        "                if simularities[min_diff_index].item() < diff_threshold:\n",
        "                    frame = cv2.putText(frame, f'{img_names_ls[min_diff_index]}',\n",
        "                                        (int(x_c + w_b/2), int(y_c)), font, fontScale, (0, 0, 200), thickness, cv2.LINE_AA)\n",
        "\n",
        "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "        # Display the frame (works in Colab)\n",
        "        # cv2_imshow(frame)\n",
        "\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "files.download('/content/result.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUCw96bKKErB"
      },
      "source": [
        "## **Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rdBvTy3B8P8p",
        "outputId": "3f64c686-2c70-4135-a8db-00f5450f4383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 4 faces, 92.1ms\n",
            "Speed: 4.6ms preprocess, 92.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.7ms\n",
            "Speed: 2.4ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.5ms\n",
            "Speed: 2.4ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 97.5ms\n",
            "Speed: 2.3ms preprocess, 97.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.8ms\n",
            "Speed: 2.9ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.9ms\n",
            "Speed: 2.1ms preprocess, 89.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.2ms\n",
            "Speed: 2.6ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 79.3ms\n",
            "Speed: 4.1ms preprocess, 79.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.0ms\n",
            "Speed: 3.1ms preprocess, 89.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.8ms\n",
            "Speed: 2.2ms preprocess, 83.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 99.7ms\n",
            "Speed: 2.2ms preprocess, 99.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.0ms\n",
            "Speed: 2.3ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.4ms\n",
            "Speed: 2.7ms preprocess, 82.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 88.9ms\n",
            "Speed: 2.2ms preprocess, 88.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.0ms\n",
            "Speed: 3.2ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 91.1ms\n",
            "Speed: 2.9ms preprocess, 91.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 90.0ms\n",
            "Speed: 2.7ms preprocess, 90.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.3ms\n",
            "Speed: 2.2ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.1ms\n",
            "Speed: 2.9ms preprocess, 80.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.5ms\n",
            "Speed: 2.1ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.1ms\n",
            "Speed: 2.1ms preprocess, 82.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 97.5ms\n",
            "Speed: 4.0ms preprocess, 97.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 90.7ms\n",
            "Speed: 2.7ms preprocess, 90.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.0ms\n",
            "Speed: 2.8ms preprocess, 85.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.0ms\n",
            "Speed: 2.2ms preprocess, 80.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.5ms\n",
            "Speed: 2.2ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.9ms\n",
            "Speed: 2.1ms preprocess, 89.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 91.3ms\n",
            "Speed: 2.1ms preprocess, 91.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 95.9ms\n",
            "Speed: 2.1ms preprocess, 95.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.2ms\n",
            "Speed: 2.8ms preprocess, 79.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.4ms\n",
            "Speed: 2.2ms preprocess, 82.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 78.0ms\n",
            "Speed: 2.2ms preprocess, 78.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 89.3ms\n",
            "Speed: 2.4ms preprocess, 89.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.0ms\n",
            "Speed: 2.9ms preprocess, 79.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 77.9ms\n",
            "Speed: 3.0ms preprocess, 77.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.0ms\n",
            "Speed: 2.1ms preprocess, 81.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.8ms\n",
            "Speed: 2.1ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 78.4ms\n",
            "Speed: 2.8ms preprocess, 78.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 94.7ms\n",
            "Speed: 1.8ms preprocess, 94.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 78.2ms\n",
            "Speed: 2.1ms preprocess, 78.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.6ms\n",
            "Speed: 2.1ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 87.4ms\n",
            "Speed: 2.1ms preprocess, 87.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 80.3ms\n",
            "Speed: 2.2ms preprocess, 80.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.3ms\n",
            "Speed: 2.8ms preprocess, 81.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 94.9ms\n",
            "Speed: 2.4ms preprocess, 94.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 75.8ms\n",
            "Speed: 2.4ms preprocess, 75.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 119.9ms\n",
            "Speed: 2.2ms preprocess, 119.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 123.8ms\n",
            "Speed: 2.2ms preprocess, 123.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 129.7ms\n",
            "Speed: 2.2ms preprocess, 129.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 114.5ms\n",
            "Speed: 2.1ms preprocess, 114.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 137.2ms\n",
            "Speed: 2.3ms preprocess, 137.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 116.9ms\n",
            "Speed: 2.2ms preprocess, 116.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 143.4ms\n",
            "Speed: 2.3ms preprocess, 143.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 122.3ms\n",
            "Speed: 2.2ms preprocess, 122.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 159.4ms\n",
            "Speed: 4.4ms preprocess, 159.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 135.1ms\n",
            "Speed: 2.5ms preprocess, 135.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 147.5ms\n",
            "Speed: 2.5ms preprocess, 147.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 111.7ms\n",
            "Speed: 3.2ms preprocess, 111.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 105.7ms\n",
            "Speed: 2.3ms preprocess, 105.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 89.4ms\n",
            "Speed: 2.1ms preprocess, 89.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.0ms\n",
            "Speed: 2.2ms preprocess, 79.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.9ms\n",
            "Speed: 2.3ms preprocess, 84.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.0ms\n",
            "Speed: 2.9ms preprocess, 79.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.0ms\n",
            "Speed: 3.0ms preprocess, 79.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 93.3ms\n",
            "Speed: 2.4ms preprocess, 93.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.8ms\n",
            "Speed: 2.6ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 79.5ms\n",
            "Speed: 2.0ms preprocess, 79.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 81.7ms\n",
            "Speed: 2.8ms preprocess, 81.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 83.3ms\n",
            "Speed: 2.2ms preprocess, 83.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.3ms\n",
            "Speed: 3.0ms preprocess, 80.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 92.4ms\n",
            "Speed: 2.1ms preprocess, 92.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.7ms\n",
            "Speed: 2.2ms preprocess, 86.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 96.3ms\n",
            "Speed: 3.8ms preprocess, 96.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.5ms\n",
            "Speed: 2.1ms preprocess, 84.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.8ms\n",
            "Speed: 3.4ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 93.0ms\n",
            "Speed: 2.2ms preprocess, 93.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.8ms\n",
            "Speed: 2.3ms preprocess, 85.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 102.9ms\n",
            "Speed: 2.4ms preprocess, 102.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.4ms\n",
            "Speed: 2.1ms preprocess, 86.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.1ms\n",
            "Speed: 2.1ms preprocess, 84.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.2ms\n",
            "Speed: 3.0ms preprocess, 86.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 88.3ms\n",
            "Speed: 2.8ms preprocess, 88.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 103.9ms\n",
            "Speed: 2.5ms preprocess, 103.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.7ms\n",
            "Speed: 2.3ms preprocess, 81.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.1ms\n",
            "Speed: 2.6ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.8ms\n",
            "Speed: 2.7ms preprocess, 86.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.1ms\n",
            "Speed: 2.1ms preprocess, 85.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 99.3ms\n",
            "Speed: 2.3ms preprocess, 99.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.3ms\n",
            "Speed: 2.2ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.6ms\n",
            "Speed: 2.2ms preprocess, 85.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.6ms\n",
            "Speed: 2.1ms preprocess, 80.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.0ms\n",
            "Speed: 2.3ms preprocess, 84.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 104.6ms\n",
            "Speed: 2.8ms preprocess, 104.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 78.7ms\n",
            "Speed: 3.9ms preprocess, 78.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.6ms\n",
            "Speed: 1.9ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.2ms\n",
            "Speed: 2.8ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 92.6ms\n",
            "Speed: 2.9ms preprocess, 92.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.4ms\n",
            "Speed: 2.4ms preprocess, 83.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 78.4ms\n",
            "Speed: 2.1ms preprocess, 78.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 87.8ms\n",
            "Speed: 1.8ms preprocess, 87.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.6ms\n",
            "Speed: 2.1ms preprocess, 84.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 79.2ms\n",
            "Speed: 2.3ms preprocess, 79.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 87.8ms\n",
            "Speed: 2.4ms preprocess, 87.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 77.4ms\n",
            "Speed: 2.0ms preprocess, 77.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.0ms\n",
            "Speed: 2.2ms preprocess, 84.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 87.5ms\n",
            "Speed: 2.2ms preprocess, 87.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.2ms\n",
            "Speed: 2.4ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 101.9ms\n",
            "Speed: 2.4ms preprocess, 101.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 94.4ms\n",
            "Speed: 3.1ms preprocess, 94.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.6ms\n",
            "Speed: 2.2ms preprocess, 81.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 108.8ms\n",
            "Speed: 2.3ms preprocess, 108.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 131.3ms\n",
            "Speed: 2.9ms preprocess, 131.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 128.6ms\n",
            "Speed: 3.2ms preprocess, 128.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 123.7ms\n",
            "Speed: 3.1ms preprocess, 123.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 124.4ms\n",
            "Speed: 2.5ms preprocess, 124.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 118.1ms\n",
            "Speed: 2.9ms preprocess, 118.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 130.0ms\n",
            "Speed: 3.2ms preprocess, 130.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 120.5ms\n",
            "Speed: 3.3ms preprocess, 120.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 130.2ms\n",
            "Speed: 2.8ms preprocess, 130.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 123.1ms\n",
            "Speed: 2.2ms preprocess, 123.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 132.2ms\n",
            "Speed: 2.1ms preprocess, 132.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 116.3ms\n",
            "Speed: 2.1ms preprocess, 116.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 100.6ms\n",
            "Speed: 2.3ms preprocess, 100.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.1ms\n",
            "Speed: 2.2ms preprocess, 81.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 93.9ms\n",
            "Speed: 3.1ms preprocess, 93.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 95.8ms\n",
            "Speed: 2.2ms preprocess, 95.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.1ms\n",
            "Speed: 2.2ms preprocess, 85.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 93.0ms\n",
            "Speed: 2.5ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.5ms\n",
            "Speed: 2.2ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 77.8ms\n",
            "Speed: 2.1ms preprocess, 77.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 88.2ms\n",
            "Speed: 2.6ms preprocess, 88.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 92.9ms\n",
            "Speed: 2.2ms preprocess, 92.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 89.9ms\n",
            "Speed: 2.4ms preprocess, 89.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 85.1ms\n",
            "Speed: 2.2ms preprocess, 85.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.6ms\n",
            "Speed: 1.7ms preprocess, 82.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.2ms\n",
            "Speed: 1.9ms preprocess, 84.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 83.7ms\n",
            "Speed: 2.8ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 96.0ms\n",
            "Speed: 2.8ms preprocess, 96.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 80.2ms\n",
            "Speed: 2.1ms preprocess, 80.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 83.6ms\n",
            "Speed: 2.2ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.0ms\n",
            "Speed: 2.7ms preprocess, 81.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.5ms\n",
            "Speed: 4.0ms preprocess, 82.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 99.9ms\n",
            "Speed: 3.0ms preprocess, 99.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.6ms\n",
            "Speed: 2.2ms preprocess, 86.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.9ms\n",
            "Speed: 2.6ms preprocess, 81.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.5ms\n",
            "Speed: 2.1ms preprocess, 79.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.0ms\n",
            "Speed: 2.3ms preprocess, 85.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.1ms\n",
            "Speed: 2.3ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 85.9ms\n",
            "Speed: 3.0ms preprocess, 85.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.6ms\n",
            "Speed: 2.1ms preprocess, 81.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.4ms\n",
            "Speed: 2.2ms preprocess, 83.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.1ms\n",
            "Speed: 2.9ms preprocess, 83.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 87.2ms\n",
            "Speed: 3.0ms preprocess, 87.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 88.4ms\n",
            "Speed: 4.7ms preprocess, 88.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.7ms\n",
            "Speed: 2.2ms preprocess, 81.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.8ms\n",
            "Speed: 2.3ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 79.3ms\n",
            "Speed: 2.2ms preprocess, 79.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 79.9ms\n",
            "Speed: 2.5ms preprocess, 79.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 94.8ms\n",
            "Speed: 2.7ms preprocess, 94.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.6ms\n",
            "Speed: 2.2ms preprocess, 80.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 92.6ms\n",
            "Speed: 2.4ms preprocess, 92.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.0ms\n",
            "Speed: 2.3ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.9ms\n",
            "Speed: 1.8ms preprocess, 82.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 105.6ms\n",
            "Speed: 2.1ms preprocess, 105.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.4ms\n",
            "Speed: 2.2ms preprocess, 89.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.4ms\n",
            "Speed: 2.3ms preprocess, 80.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.0ms\n",
            "Speed: 2.9ms preprocess, 82.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 83.0ms\n",
            "Speed: 2.6ms preprocess, 83.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.7ms\n",
            "Speed: 2.9ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.9ms\n",
            "Speed: 2.1ms preprocess, 80.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 89.0ms\n",
            "Speed: 3.1ms preprocess, 89.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.7ms\n",
            "Speed: 2.7ms preprocess, 84.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.3ms\n",
            "Speed: 2.4ms preprocess, 86.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 81.8ms\n",
            "Speed: 1.9ms preprocess, 81.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.2ms\n",
            "Speed: 3.5ms preprocess, 80.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 121.2ms\n",
            "Speed: 3.2ms preprocess, 121.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 118.9ms\n",
            "Speed: 2.3ms preprocess, 118.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 118.1ms\n",
            "Speed: 2.1ms preprocess, 118.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 118.5ms\n",
            "Speed: 3.4ms preprocess, 118.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 121.5ms\n",
            "Speed: 2.8ms preprocess, 121.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 129.7ms\n",
            "Speed: 2.5ms preprocess, 129.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 131.6ms\n",
            "Speed: 2.4ms preprocess, 131.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 128.2ms\n",
            "Speed: 2.2ms preprocess, 128.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 134.2ms\n",
            "Speed: 3.4ms preprocess, 134.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 122.0ms\n",
            "Speed: 2.2ms preprocess, 122.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 88.9ms\n",
            "Speed: 2.8ms preprocess, 88.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 80.3ms\n",
            "Speed: 2.2ms preprocess, 80.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 80.4ms\n",
            "Speed: 2.2ms preprocess, 80.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 93.4ms\n",
            "Speed: 2.8ms preprocess, 93.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.5ms\n",
            "Speed: 2.4ms preprocess, 82.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 80.6ms\n",
            "Speed: 2.2ms preprocess, 80.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.0ms\n",
            "Speed: 2.2ms preprocess, 83.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.1ms\n",
            "Speed: 2.2ms preprocess, 84.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.6ms\n",
            "Speed: 2.6ms preprocess, 81.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 79.8ms\n",
            "Speed: 2.2ms preprocess, 79.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 80.2ms\n",
            "Speed: 2.2ms preprocess, 80.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 76.1ms\n",
            "Speed: 2.2ms preprocess, 76.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.6ms\n",
            "Speed: 2.8ms preprocess, 83.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.8ms\n",
            "Speed: 2.6ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 95.5ms\n",
            "Speed: 2.6ms preprocess, 95.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 78.2ms\n",
            "Speed: 2.1ms preprocess, 78.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.0ms\n",
            "Speed: 2.4ms preprocess, 82.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 94.0ms\n",
            "Speed: 2.3ms preprocess, 94.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.7ms\n",
            "Speed: 2.2ms preprocess, 81.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 85.4ms\n",
            "Speed: 2.3ms preprocess, 85.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 97.8ms\n",
            "Speed: 2.2ms preprocess, 97.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.1ms\n",
            "Speed: 1.8ms preprocess, 83.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 85.9ms\n",
            "Speed: 2.4ms preprocess, 85.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 102.9ms\n",
            "Speed: 2.8ms preprocess, 102.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.4ms\n",
            "Speed: 2.2ms preprocess, 82.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 83.0ms\n",
            "Speed: 2.6ms preprocess, 83.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.2ms\n",
            "Speed: 3.8ms preprocess, 81.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.8ms\n",
            "Speed: 2.9ms preprocess, 81.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.7ms\n",
            "Speed: 1.9ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 90.1ms\n",
            "Speed: 1.9ms preprocess, 90.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 82.8ms\n",
            "Speed: 2.7ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 97.1ms\n",
            "Speed: 2.6ms preprocess, 97.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 80.7ms\n",
            "Speed: 2.3ms preprocess, 80.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.5ms\n",
            "Speed: 2.4ms preprocess, 84.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.2ms\n",
            "Speed: 2.0ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.7ms\n",
            "Speed: 2.3ms preprocess, 81.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 85.4ms\n",
            "Speed: 2.2ms preprocess, 85.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 96.5ms\n",
            "Speed: 2.5ms preprocess, 96.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 84.1ms\n",
            "Speed: 2.1ms preprocess, 84.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 87.1ms\n",
            "Speed: 2.2ms preprocess, 87.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 86.3ms\n",
            "Speed: 2.6ms preprocess, 86.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 87.0ms\n",
            "Speed: 2.2ms preprocess, 87.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 90.2ms\n",
            "Speed: 2.5ms preprocess, 90.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 84.6ms\n",
            "Speed: 4.0ms preprocess, 84.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 84.2ms\n",
            "Speed: 2.2ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 88.1ms\n",
            "Speed: 2.3ms preprocess, 88.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 97.1ms\n",
            "Speed: 2.4ms preprocess, 97.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 88.0ms\n",
            "Speed: 2.7ms preprocess, 88.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 78.1ms\n",
            "Speed: 2.0ms preprocess, 78.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 98.6ms\n",
            "Speed: 2.3ms preprocess, 98.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 114.0ms\n",
            "Speed: 2.2ms preprocess, 114.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 102.1ms\n",
            "Speed: 2.3ms preprocess, 102.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 80.2ms\n",
            "Speed: 2.2ms preprocess, 80.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 122.2ms\n",
            "Speed: 2.3ms preprocess, 122.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 126.1ms\n",
            "Speed: 2.2ms preprocess, 126.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 116.4ms\n",
            "Speed: 2.1ms preprocess, 116.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 116.1ms\n",
            "Speed: 2.1ms preprocess, 116.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 119.3ms\n",
            "Speed: 2.2ms preprocess, 119.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 117.8ms\n",
            "Speed: 2.4ms preprocess, 117.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 122.1ms\n",
            "Speed: 2.2ms preprocess, 122.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 120.7ms\n",
            "Speed: 2.2ms preprocess, 120.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 133.0ms\n",
            "Speed: 2.3ms preprocess, 133.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 163.5ms\n",
            "Speed: 2.1ms preprocess, 163.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 118.9ms\n",
            "Speed: 2.9ms preprocess, 118.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 80.4ms\n",
            "Speed: 3.2ms preprocess, 80.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 86.6ms\n",
            "Speed: 2.2ms preprocess, 86.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 83.0ms\n",
            "Speed: 2.9ms preprocess, 83.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 85.0ms\n",
            "Speed: 2.6ms preprocess, 85.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 82.0ms\n",
            "Speed: 3.2ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 83.9ms\n",
            "Speed: 2.2ms preprocess, 83.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 faces, 84.5ms\n",
            "Speed: 2.8ms preprocess, 84.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 86.2ms\n",
            "Speed: 2.3ms preprocess, 86.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 103.7ms\n",
            "Speed: 2.4ms preprocess, 103.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.9ms\n",
            "Speed: 2.4ms preprocess, 84.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 82.3ms\n",
            "Speed: 2.6ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 faces, 84.9ms\n",
            "Speed: 2.7ms preprocess, 84.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 faces, 81.1ms\n",
            "Speed: 3.7ms preprocess, 81.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 face, 90.7ms\n",
            "Speed: 2.2ms preprocess, 90.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 face, 82.9ms\n",
            "Speed: 2.5ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c7679e7d-e396-4ee7-9648-d5949062f000\", \"result.mp4\", 7723165)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Open the camera or video file\n",
        "cap = cv2.VideoCapture(\"/content/subway.mp4\")\n",
        "\n",
        "# Initialize other variables\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "# cam_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "cam_fps = 15\n",
        "\n",
        "# Output video\n",
        "out = cv2.VideoWriter('/content/result.mp4', cv2.VideoWriter_fourcc(*'mp4v'), cam_fps, (frame_width, frame_height))\n",
        "\n",
        "# Font settings\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 0.8\n",
        "thickness = 2\n",
        "\n",
        "# simmularity threshold\n",
        "diff_threshold = 0.7\n",
        "# Define buffer to store last n crops and their identities for each person\n",
        "buffer_size = 5\n",
        "# Dictionary to store buffer per person {id: {'embeddings': deque(), 'labels': deque()}}\n",
        "person_buffers = {}\n",
        "face_id = None\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "\n",
        "        results = face_detector.track(frame, persist=True, device=device)\n",
        "\n",
        "        for r in results[0]:\n",
        "            if 0.6 < r.boxes.conf.item():\n",
        "                points = r.keypoints.xy.cpu().numpy()[0]\n",
        "                x_c, y_c, w_b, h_b = r.boxes.xywh.cpu().numpy()[0]\n",
        "                x_max = int(x_c + (w_b / 2))\n",
        "                x_min = int(x_c - (w_b / 2))\n",
        "                y_max = int(y_c + (h_b / 2))\n",
        "                y_min = int(y_c - (h_b / 2))\n",
        "                \n",
        "                cropped_face = frame[y_min:y_max, x_min:x_max]\n",
        "                cropped_face_embedding = face_crop_embedder(cropped_face)\n",
        "                # Get tracking ID (assuming r.track_id exists)\n",
        "                if r.boxes.id is not None:\n",
        "                  face_id = r.boxes.id.item()\n",
        "                  # print(\"face_id: \", face_id)\n",
        "                # Initialize buffer for the person if not already in the dictionary\n",
        "                if face_id not in person_buffers:\n",
        "                    person_buffers[face_id] = {'labels': deque(maxlen=buffer_size)}\n",
        "\n",
        "                # Calculate difference with database and determine identity\n",
        "                simularities = (Data_Base*cropped_face_embedding).sum(dim=1)\n",
        "                # print(\"simularities: \", simularities)\n",
        "                simularities = 1 - simularities\n",
        "                # print(\"1 - simularities: \", simularities)\n",
        "                min_diff_index = torch.argmin(simularities)\n",
        "\n",
        "                # If confidence of match is high, declare the identity\n",
        "                if simularities[min_diff_index].item() < diff_threshold:\n",
        "                    identity = img_names_ls[min_diff_index].split(\"_\")[0]\n",
        "                    # Store the identity in the buffer\n",
        "                    person_buffers[face_id]['labels'].append(identity)  # Store the identity in buffer\n",
        "\n",
        "\n",
        "                # Check if buffer is full (3 consecutive crops)\n",
        "                if len(person_buffers[face_id]['labels']) == buffer_size:\n",
        "                    # Perform voting on the labels\n",
        "                    most_common_identity = Counter(person_buffers[face_id]['labels']).most_common(1)[0][0]\n",
        "                    # Display the most common identity\n",
        "                    frame = cv2.putText(frame, f'{most_common_identity}', (int(x_c + w_b / 2), int(y_c)),\n",
        "                                        font, fontScale, (0, 0, 200), thickness, cv2.LINE_AA)\n",
        "\n",
        "                # Draw rectangle around the face\n",
        "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 10), 2)\n",
        "                frame = cv2.putText(frame, f'face_id: {face_id}', (int(1 + x_c + w_b / 2), int(50 + y_c)),\n",
        "                                        font, 0.6, (212, 122, 66), 1, cv2.LINE_AA)\n",
        "        out.write(frame)\n",
        "\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "files.download('/content/result.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ39bfeDNZIe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18974c3603544632a35480031e9f0dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28e6e2a5593e41848e686d110ff79b28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294e2c9a484c48f282f2c4eef931149d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353869a0cc2b458ba68aa7c9469a3a68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c536faa49694d948a4a71c0affd95b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ce5e005413470881aaeca1225ab9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47bab7a2395c46c2a55adb5d07612d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fcbf9e3789d401fbec572cc12f0fe98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28e6e2a5593e41848e686d110ff79b28",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0051db1596b4fabaa20f177dbb6297b",
            "value": 111898327
          }
        },
        "5fd04b161634444e99254acc7450b813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_294e2c9a484c48f282f2c4eef931149d",
            "placeholder": "​",
            "style": "IPY_MODEL_fc73dd8129de4241a67d98c24e6a15b4",
            "value": " 107M/107M [00:01&lt;00:00, 89.6MB/s]"
          }
        },
        "7d918e81445740059b871a95860d4cab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802aedba2a474837a1366a681c66ab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5bc0503e5d04c39abb4ab10d5097b6b",
            "placeholder": "​",
            "style": "IPY_MODEL_9a6ff4074032408c80afada76f7074b1",
            "value": "100%"
          }
        },
        "9a6ff4074032408c80afada76f7074b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dae755a834143cb9babaa1ba0265620": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16610475c084149a0d20c89b55ec85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353869a0cc2b458ba68aa7c9469a3a68",
            "placeholder": "​",
            "style": "IPY_MODEL_46ce5e005413470881aaeca1225ab9eb",
            "value": " 107M/107M [00:00&lt;00:00, 280MB/s]"
          }
        },
        "aebca98d303642378f3e35997a1ac7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9f33df4b9bd453ba03d0b59e9310565",
              "IPY_MODEL_5fcbf9e3789d401fbec572cc12f0fe98",
              "IPY_MODEL_5fd04b161634444e99254acc7450b813"
            ],
            "layout": "IPY_MODEL_9dae755a834143cb9babaa1ba0265620"
          }
        },
        "bf2b53d914484a4c8c844e9c5db430a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d918e81445740059b871a95860d4cab",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9e06fac4bd94cbca0c0a1bd88e36e62",
            "value": 111898327
          }
        },
        "c0051db1596b4fabaa20f177dbb6297b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9e06fac4bd94cbca0c0a1bd88e36e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7bc9694f3c242959e7ede7b3dbab621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_802aedba2a474837a1366a681c66ab34",
              "IPY_MODEL_bf2b53d914484a4c8c844e9c5db430a7",
              "IPY_MODEL_a16610475c084149a0d20c89b55ec85f"
            ],
            "layout": "IPY_MODEL_3c536faa49694d948a4a71c0affd95b4"
          }
        },
        "d9f33df4b9bd453ba03d0b59e9310565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47bab7a2395c46c2a55adb5d07612d3f",
            "placeholder": "​",
            "style": "IPY_MODEL_18974c3603544632a35480031e9f0dd6",
            "value": "100%"
          }
        },
        "f5bc0503e5d04c39abb4ab10d5097b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc73dd8129de4241a67d98c24e6a15b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
